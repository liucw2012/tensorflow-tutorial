{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed TensorFlow\n",
    "\n",
    "This document shows how to create a cluster of TensorFlow servers, and how to\n",
    "distribute a computation graph across that cluster. We assume that you are\n",
    "familiar with the [basic concepts](https://tensorflow.org/guide/low_level_intro)\n",
    "of writing low level TensorFlow programs.\n",
    "\n",
    "## Hello distributed TensorFlow!\n",
    "\n",
    "To see a simple TensorFlow cluster in action, execute the following:\n",
    "\n",
    "```shell\n",
    "# Start a TensorFlow server as a single-process \"cluster\".\n",
    "$ python\n",
    ">>> import tensorflow as tf\n",
    ">>> c = tf.constant(\"Hello, distributed TensorFlow!\")\n",
    ">>> server = tf.train.Server.create_local_server()\n",
    ">>> sess = tf.Session(server.target)  # Create a session on the server.\n",
    ">>> sess.run(c)\n",
    "'Hello, distributed TensorFlow!'\n",
    "```\n",
    "\n",
    "The\n",
    "`tf.train.Server.create_local_server`\n",
    "method creates a single-process cluster, with an in-process server.\n",
    "\n",
    "## Create a cluster\n",
    "\n",
    "[![Distributed TensorFlow (TensorFlow Dev Summit 2017)](http://img.youtube.com/vi/la_M6bCV91M/0.jpg)](https://www.youtube.com/embed/la_M6bCV91M \"Distributed TensorFlow (TensorFlow Dev Summit 2017)\")\n",
    "\n",
    "A TensorFlow \"cluster\" is a set of \"tasks\" that participate in the distributed\n",
    "execution of a TensorFlow graph. Each task is associated with a TensorFlow\n",
    "\"server\", which contains a \"master\" that can be used to create sessions, and a\n",
    "\"worker\" that executes operations in the graph.  A cluster can also be divided\n",
    "into one or more \"jobs\", where each job contains one or more tasks.\n",
    "\n",
    "To create a cluster, you start one TensorFlow server per task in the cluster.\n",
    "Each task typically runs on a different machine, but you can run multiple tasks\n",
    "on the same machine (e.g. to control different GPU devices). In each task, do\n",
    "the following:\n",
    "\n",
    "1.  **Create a `tf.train.ClusterSpec`** that describes all of the tasks\n",
    "    in the cluster. This should be the same for each task.\n",
    "\n",
    "2.  **Create a `tf.train.Server`**, passing the `tf.train.ClusterSpec` to\n",
    "    the constructor, and identifying the local task with a job name\n",
    "    and task index.\n",
    "\n",
    "\n",
    "### Create a `tf.train.ClusterSpec` to describe the cluster\n",
    "\n",
    "The cluster specification dictionary maps job names to lists of network\n",
    "addresses. Pass this dictionary to\n",
    "the `tf.train.ClusterSpec`\n",
    "constructor.  For example:\n",
    "\n",
    "<table>\n",
    "  <tr><th><code>tf.train.ClusterSpec</code> construction</th><th>Available tasks</th>\n",
    "  <tr>\n",
    "    <td><pre>\n",
    "tf.train.ClusterSpec({\"local\": [\"localhost:2222\", \"localhost:2223\"]})\n",
    "</pre></td>\n",
    "    <td><code>/job:local/task:0</code><br/><code>/job:local/task:1</code></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><pre>\n",
    "tf.train.ClusterSpec({\n",
    "    \"worker\": [\n",
    "        \"worker0.example.com:2222\",\n",
    "        \"worker1.example.com:2222\",\n",
    "        \"worker2.example.com:2222\"\n",
    "    ],\n",
    "    \"ps\": [\n",
    "        \"ps0.example.com:2222\",\n",
    "        \"ps1.example.com:2222\"\n",
    "    ]})\n",
    "</pre></td><td><code>/job:worker/task:0</code><br/><code>/job:worker/task:1</code><br/><code>/job:worker/task:2</code><br/><code>/job:ps/task:0</code><br/><code>/job:ps/task:1</code></td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "### Create a `tf.train.Server` instance in each task\n",
    "\n",
    "A `tf.train.Server` object contains a\n",
    "set of local devices, a set of connections to other tasks in its\n",
    "`tf.train.ClusterSpec`, and a\n",
    "`tf.Session` that can use these\n",
    "to perform a distributed computation. Each server is a member of a specific\n",
    "named job and has a task index within that job.  A server can communicate with\n",
    "any other server in the cluster.\n",
    "\n",
    "For example, to launch a cluster with two servers running on `localhost:2222`\n",
    "and `localhost:2223`, run the following snippets in two different processes on\n",
    "the local machine:\n",
    "\n",
    "```python\n",
    "# In task 0:\n",
    "cluster = tf.train.ClusterSpec({\"local\": [\"localhost:2222\", \"localhost:2223\"]})\n",
    "server = tf.train.Server(cluster, job_name=\"local\", task_index=0)\n",
    "```\n",
    "```python\n",
    "# In task 1:\n",
    "cluster = tf.train.ClusterSpec({\"local\": [\"localhost:2222\", \"localhost:2223\"]})\n",
    "server = tf.train.Server(cluster, job_name=\"local\", task_index=1)\n",
    "```\n",
    "\n",
    "**Note:** Manually specifying these cluster specifications can be tedious,\n",
    "especially for large clusters. We are working on tools for launching tasks\n",
    "programmatically, e.g. using a cluster manager like\n",
    "[Kubernetes](http://kubernetes.io). If there are particular cluster managers for\n",
    "which you'd like to see support, please raise a\n",
    "[GitHub issue](https://github.com/tensorflow/tensorflow/issues).\n",
    "\n",
    "## Specifying distributed devices in your model\n",
    "\n",
    "To place operations on a particular process, you can use the same\n",
    "`tf.device`\n",
    "function that is used to specify whether ops run on the CPU or GPU. For example:\n",
    "\n",
    "```python\n",
    "with tf.device(\"/job:ps/task:0\"):\n",
    "  weights_1 = tf.Variable(...)\n",
    "  biases_1 = tf.Variable(...)\n",
    "\n",
    "with tf.device(\"/job:ps/task:1\"):\n",
    "  weights_2 = tf.Variable(...)\n",
    "  biases_2 = tf.Variable(...)\n",
    "\n",
    "with tf.device(\"/job:worker/task:7\"):\n",
    "  input, labels = ...\n",
    "  layer_1 = tf.nn.relu(tf.matmul(input, weights_1) + biases_1)\n",
    "  logits = tf.nn.relu(tf.matmul(layer_1, weights_2) + biases_2)\n",
    "  # ...\n",
    "  train_op = ...\n",
    "\n",
    "with tf.Session(\"grpc://worker7.example.com:2222\") as sess:\n",
    "  for _ in range(10000):\n",
    "    sess.run(train_op)\n",
    "```\n",
    "\n",
    "In the above example, the variables are created on two tasks in the `ps` job,\n",
    "and the compute-intensive part of the model is created in the `worker`\n",
    "job. TensorFlow will insert the appropriate data transfers between the jobs\n",
    "(from `ps` to `worker` for the forward pass, and from `worker` to `ps` for\n",
    "applying gradients).\n",
    "\n",
    "## Replicated training\n",
    "\n",
    "A common training configuration, called \"data parallelism,\" involves multiple\n",
    "tasks in a `worker` job training the same model on different mini-batches of\n",
    "data, updating shared parameters hosted in one or more tasks in a `ps`\n",
    "job. All tasks typically run on different machines. There are many ways to\n",
    "specify this structure in TensorFlow, and we are building libraries that will\n",
    "simplify the work of specifying a replicated model. Possible approaches include:\n",
    "\n",
    "* **In-graph replication.** In this approach, the client builds a single\n",
    "  `tf.Graph` that contains one set of parameters (in `tf.Variable` nodes pinned\n",
    "  to `/job:ps`); and multiple copies of the compute-intensive part of the model,\n",
    "  each pinned to a different task in `/job:worker`.\n",
    "\n",
    "* **Between-graph replication.** In this approach, there is a separate client\n",
    "  for each `/job:worker` task, typically in the same process as the worker\n",
    "  task. Each client builds a similar graph containing the parameters (pinned to\n",
    "  `/job:ps` as before using\n",
    "  `tf.train.replica_device_setter`\n",
    "  to map them deterministically to the same tasks); and a single copy of the\n",
    "  compute-intensive part of the model, pinned to the local task in\n",
    "  `/job:worker`.\n",
    "\n",
    "* **Asynchronous training.** In this approach, each replica of the graph has an\n",
    "  independent training loop that executes without coordination. It is compatible\n",
    "  with both forms of replication above.\n",
    "\n",
    "* **Synchronous training.** In this approach, all of the replicas read the same\n",
    "  values for the current parameters, compute gradients in parallel, and then\n",
    "  apply them together. It is compatible with in-graph replication (e.g. using\n",
    "  gradient averaging as in the\n",
    "  [CIFAR-10 multi-GPU trainer](https://www.tensorflow.org/code/tensorflow_models/tutorials/image/cifar10/cifar10_multi_gpu_train.py)),\n",
    "  and between-graph replication (e.g. using the\n",
    "  `tf.train.SyncReplicasOptimizer`).\n",
    "\n",
    "### Putting it all together: example trainer program\n",
    "\n",
    "The following code shows the skeleton of a distributed trainer program,\n",
    "implementing **between-graph replication** and **asynchronous training**. It\n",
    "includes the code for the parameter server and worker tasks.\n",
    "\n",
    "```python\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "FLAGS = None\n",
    "\n",
    "\n",
    "def main(_):\n",
    "  ps_hosts = FLAGS.ps_hosts.split(\",\")\n",
    "  worker_hosts = FLAGS.worker_hosts.split(\",\")\n",
    "\n",
    "  # Create a cluster from the parameter server and worker hosts.\n",
    "  cluster = tf.train.ClusterSpec({\"ps\": ps_hosts, \"worker\": worker_hosts})\n",
    "\n",
    "  # Create and start a server for the local task.\n",
    "  server = tf.train.Server(cluster,\n",
    "                           job_name=FLAGS.job_name,\n",
    "                           task_index=FLAGS.task_index)\n",
    "\n",
    "  if FLAGS.job_name == \"ps\":\n",
    "    server.join()\n",
    "  elif FLAGS.job_name == \"worker\":\n",
    "\n",
    "    # Assigns ops to the local worker by default.\n",
    "    with tf.device(tf.train.replica_device_setter(\n",
    "        worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\n",
    "        cluster=cluster)):\n",
    "\n",
    "      # Build model...\n",
    "      loss = ...\n",
    "      global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "\n",
    "      train_op = tf.train.AdagradOptimizer(0.01).minimize(\n",
    "          loss, global_step=global_step)\n",
    "\n",
    "    # The StopAtStepHook handles stopping after running given steps.\n",
    "    hooks=[tf.train.StopAtStepHook(last_step=1000000)]\n",
    "\n",
    "    # The MonitoredTrainingSession takes care of session initialization,\n",
    "    # restoring from a checkpoint, saving to a checkpoint, and closing when done\n",
    "    # or an error occurs.\n",
    "    with tf.train.MonitoredTrainingSession(master=server.target,\n",
    "                                           is_chief=(FLAGS.task_index == 0),\n",
    "                                           checkpoint_dir=\"/tmp/train_logs\",\n",
    "                                           hooks=hooks) as mon_sess:\n",
    "      while not mon_sess.should_stop():\n",
    "        # Run a training step asynchronously.\n",
    "        # See `tf.train.SyncReplicasOptimizer` for additional details on how to\n",
    "        # perform *synchronous* training.\n",
    "        # mon_sess.run handles AbortedError in case of preempted PS.\n",
    "        mon_sess.run(train_op)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.register(\"type\", \"bool\", lambda v: v.lower() == \"true\")\n",
    "  # Flags for defining the tf.train.ClusterSpec\n",
    "  parser.add_argument(\n",
    "      \"--ps_hosts\",\n",
    "      type=str,\n",
    "      default=\"\",\n",
    "      help=\"Comma-separated list of hostname:port pairs\"\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      \"--worker_hosts\",\n",
    "      type=str,\n",
    "      default=\"\",\n",
    "      help=\"Comma-separated list of hostname:port pairs\"\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      \"--job_name\",\n",
    "      type=str,\n",
    "      default=\"\",\n",
    "      help=\"One of 'ps', 'worker'\"\n",
    "  )\n",
    "  # Flags for defining the tf.train.Server\n",
    "  parser.add_argument(\n",
    "      \"--task_index\",\n",
    "      type=int,\n",
    "      default=0,\n",
    "      help=\"Index of task within the job\"\n",
    "  )\n",
    "  FLAGS, unparsed = parser.parse_known_args()\n",
    "  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n",
    "```\n",
    "\n",
    "To start the trainer with two parameter servers and two workers, use the\n",
    "following command line (assuming the script is called `trainer.py`):\n",
    "\n",
    "```shell\n",
    "# On ps0.example.com:\n",
    "$ python trainer.py \\\n",
    "     --ps_hosts=ps0.example.com:2222,ps1.example.com:2222 \\\n",
    "     --worker_hosts=worker0.example.com:2222,worker1.example.com:2222 \\\n",
    "     --job_name=ps --task_index=0\n",
    "# On ps1.example.com:\n",
    "$ python trainer.py \\\n",
    "     --ps_hosts=ps0.example.com:2222,ps1.example.com:2222 \\\n",
    "     --worker_hosts=worker0.example.com:2222,worker1.example.com:2222 \\\n",
    "     --job_name=ps --task_index=1\n",
    "# On worker0.example.com:\n",
    "$ python trainer.py \\\n",
    "     --ps_hosts=ps0.example.com:2222,ps1.example.com:2222 \\\n",
    "     --worker_hosts=worker0.example.com:2222,worker1.example.com:2222 \\\n",
    "     --job_name=worker --task_index=0\n",
    "# On worker1.example.com:\n",
    "$ python trainer.py \\\n",
    "     --ps_hosts=ps0.example.com:2222,ps1.example.com:2222 \\\n",
    "     --worker_hosts=worker0.example.com:2222,worker1.example.com:2222 \\\n",
    "     --job_name=worker --task_index=1\n",
    "```\n",
    "\n",
    "## Glossary\n",
    "\n",
    "**Client**\n",
    "\n",
    "A client is typically a program that builds a TensorFlow graph and constructs a\n",
    "`tensorflow::Session` to interact with a cluster. Clients are typically written\n",
    "in Python or C++. A single client process can directly interact with multiple\n",
    "TensorFlow servers (see \"Replicated training\" above), and a single server can\n",
    "serve multiple clients.\n",
    "\n",
    "**Cluster**\n",
    "\n",
    "A TensorFlow cluster comprises one or more \"jobs\", each divided into lists of\n",
    "one or more \"tasks\". A cluster is typically dedicated to a particular high-level\n",
    "objective, such as training a neural network, using many machines in parallel. A\n",
    "cluster is defined by\n",
    "a `tf.train.ClusterSpec` object.\n",
    "\n",
    "**Job**\n",
    "\n",
    "A job comprises a list of \"tasks\", which typically serve a common purpose.\n",
    "For example, a job named `ps` (for \"parameter server\") typically hosts nodes\n",
    "that store and update variables; while a job named `worker` typically hosts\n",
    "stateless nodes that perform compute-intensive tasks. The tasks in a job\n",
    "typically run on different machines. The set of job roles is flexible:\n",
    "for example, a `worker` may maintain some state.\n",
    "\n",
    "**Master service**\n",
    "\n",
    "An RPC service that provides remote access to a set of distributed devices,\n",
    "and acts as a session target. The master service implements the\n",
    "`tensorflow::Session` interface, and is responsible for coordinating work across\n",
    "one or more \"worker services\". All TensorFlow servers implement the master\n",
    "service.\n",
    "\n",
    "**Task**\n",
    "\n",
    "A task corresponds to a specific TensorFlow server, and typically corresponds\n",
    "to a single process. A task belongs to a particular \"job\" and is identified by\n",
    "its index within that job's list of tasks.\n",
    "\n",
    "**TensorFlow server** A process running\n",
    "a `tf.train.Server` instance, which is\n",
    "a member of a cluster, and exports a \"master service\" and \"worker service\".\n",
    "\n",
    "**Worker service**\n",
    "\n",
    "An RPC service that executes parts of a TensorFlow graph using its local devices.\n",
    "A worker service implements [worker_service.proto](https://www.tensorflow.org/code/tensorflow/core/protobuf/worker_service.proto).\n",
    "All TensorFlow servers implement the worker service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow on Hadoop\n",
    "\n",
    "This document describes how to run TensorFlow on Hadoop using HDFS. You should\n",
    "know how to [import data](https://tensorflow.org/guide/datasets).\n",
    "\n",
    "## HDFS\n",
    "\n",
    "To use HDFS with TensorFlow, Use HDFS paths for reading and writing data, for\n",
    "example:\n",
    "\n",
    "```python\n",
    "filename_queue = tf.train.string_input_producer([\n",
    "    \"hdfs://namenode:8020/path/to/file1.csv\",\n",
    "    \"hdfs://namenode:8020/path/to/file2.csv\",\n",
    "])\n",
    "```\n",
    "\n",
    "To use the `namenode` specified in your HDFS configuration files, change the file\n",
    "prefix to `hdfs://default/`.\n",
    "\n",
    "Set the following environment variables:\n",
    "\n",
    "* `JAVA_HOME` —Location of the Java installation.\n",
    "* `HADOOP_HDFS_HOME` —Location of the HDFS installation. The variable is optional\n",
    "  if `libhdfs.so` is available in `LD_LIBRARY_PATH`. This can also be set using:\n",
    "  \n",
    "  ```shell\n",
    "  source ${HADOOP_HOME}/libexec/hadoop-config.sh\n",
    "  ```\n",
    "\n",
    "* `LD_LIBRARY_PATH` —Include the path to `libjvm.so` and, optionally, the path to\n",
    "  `libhdfs.so`, if your Hadoop distribution did not install `libhdfs.so` in\n",
    "  `${HADOOP_HDFS_HOME}/lib/native`. On Linux:\n",
    "\n",
    "  ```shell\n",
    "  export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:${JAVA_HOME}/jre/lib/amd64/server\n",
    "  ```\n",
    "\n",
    "* `CLASSPATH` —The Hadoop jars must be added to the class path before running\n",
    "  TensorFlow. It's not enough to set CLASSPATH using:\n",
    "  `${HADOOP_HOME}/libexec/hadoop-config.sh`. Globs must be expanded, as described\n",
    "  in the `libhdfs` documentation:\n",
    "\n",
    "  ```shell\n",
    "  CLASSPATH=$(${HADOOP_HOME}/bin/hadoop classpath --glob) python your_script.py\n",
    "  ```\n",
    "\n",
    "If the Hadoop cluster is in *secure mode*, set the following environment variable:\n",
    "\n",
    "* `KRB5CCNAME` —Path of Kerberos ticket cache file. For example:\n",
    "\n",
    "  ```shell\n",
    "  export KRB5CCNAME=/tmp/krb5cc_10002\n",
    "  ```\n",
    "\n",
    "If using [Distributed TensorFlow](distributed.md), all workers must have Hadoop\n",
    "installed and the environment variables set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow on S3\n",
    "\n",
    "Tensorflow supports reading and writing data to S3. S3 is an object storage API\n",
    "which is nearly ubiquitous, and can help in situations where data must accessed\n",
    "by multiple actors, such as in distributed training.\n",
    "\n",
    "This document guides you through the required setup, and provides examples on\n",
    "usage.\n",
    "\n",
    "## Configuration\n",
    "\n",
    "When reading or writing data on S3 with your TensorFlow program, the behavior\n",
    "can be controlled by various environmental variables:\n",
    "\n",
    "*   **AWS_REGION**: By default, regional endpoint is used for S3, with region\n",
    "    controlled by `AWS_REGION`. If `AWS_REGION` is not specified, then\n",
    "    `us-east-1` is used.\n",
    "*   **S3_ENDPOINT**: The endpoint could be overridden explicitly with\n",
    "    `S3_ENDPOINT` specified.\n",
    "*   **S3_USE_HTTPS**: HTTPS is used to access S3 by default, unless\n",
    "    `S3_USE_HTTPS=0`.\n",
    "*   **S3_VERIFY_SSL**: If HTTPS is used, SSL verification could be disabled\n",
    "    with `S3_VERIFY_SSL=0`.\n",
    "\n",
    "To read or write objects in a bucket that is not publicly accessible,\n",
    "AWS credentials must be provided through one of the following methods:\n",
    "\n",
    "*   Set credentials in the AWS credentials profile file on the local system,\n",
    "    located at: `~/.aws/credentials` on Linux, macOS, or Unix, or\n",
    "    `C:\\Users\\USERNAME\\.aws\\credentials` on Windows.\n",
    "*   Set the `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` environment\n",
    "    variables.\n",
    "*   If TensorFlow is deployed on an EC2 instance, specify an IAM role and then\n",
    "    give the EC2 instance access to that role.\n",
    "\n",
    "## Example Setup\n",
    "\n",
    "Using the above information, we can configure Tensorflow to communicate to an S3\n",
    "endpoint by setting the following environment variables:\n",
    "\n",
    "```bash\n",
    "AWS_ACCESS_KEY_ID=XXXXX                 # Credentials only needed if connecting to a private endpoint\n",
    "AWS_SECRET_ACCESS_KEY=XXXXX\n",
    "AWS_REGION=us-east-1                    # Region for the S3 bucket, this is not always needed. Default is us-east-1.\n",
    "S3_ENDPOINT=s3.us-east-1.amazonaws.com  # The S3 API Endpoint to connect to. This is specified in a HOST:PORT format.\n",
    "S3_USE_HTTPS=1                          # Whether or not to use HTTPS. Disable with 0.\n",
    "S3_VERIFY_SSL=1                         # If HTTPS is used, controls if SSL should be enabled. Disable with 0.\n",
    "```\n",
    "\n",
    "## Usage\n",
    "\n",
    "Once setup is completed, Tensorflow can interact with S3 in a variety of ways.\n",
    "Anywhere there is a Tensorflow IO function, an S3 URL can be used.\n",
    "\n",
    "### Smoke Test\n",
    "\n",
    "To test your setup, stat a file:\n",
    "\n",
    "```python\n",
    "from tensorflow.python.lib.io import file_io\n",
    "print file_io.stat('s3://bucketname/path/')\n",
    "```\n",
    "\n",
    "You should see output similar to this:\n",
    "\n",
    "```console\n",
    "<tensorflow.python.pywrap_tensorflow_internal.FileStatistics; proxy of <Swig Object of type 'tensorflow::FileStatistics *' at 0x10c2171b0> >\n",
    "```\n",
    "\n",
    "### Reading Data\n",
    "\n",
    "```python\n",
    "filenames = [\"s3://bucketname/path/to/file1.tfrecord\",\n",
    "             \"s3://bucketname/path/to/file2.tfrecord\"]\n",
    "dataset = tf.data.TFRecordDataset(filenames)\n",
    "```\n",
    "\n",
    "### Tensorflow Tools\n",
    "\n",
    "Many Tensorflow tools, such as Tensorboard or model serving, can also take S3\n",
    "URLS as arguments:\n",
    "\n",
    "```bash\n",
    "tensorboard --logdir s3://bucketname/path/to/model/\n",
    "tensorflow_model_server --port=9000 --model_name=model --model_base_path=s3://bucketname/path/to/model/export/\n",
    "```\n",
    "\n",
    "This enables an end to end workflow using S3 for all data needs.\n",
    "\n",
    "## S3 Endpoint Implementations\n",
    "\n",
    "S3 was invented by Amazon, but the S3 API has spread in popularity and has\n",
    "several implementations. The following implementations have passed basic\n",
    "compatibility tests:\n",
    "\n",
    "* [Amazon S3](https://aws.amazon.com/s3/)\n",
    "* [Google Storage](https://cloud.google.com/storage/docs/interoperability)\n",
    "* [Minio](https://www.minio.io/kubernetes.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
