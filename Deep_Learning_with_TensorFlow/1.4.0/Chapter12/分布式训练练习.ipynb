{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU 基本操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d: /device:GPU:1\n",
      "d: /device:GPU:0\n",
      "c: [<tf.Tensor 'MatMul:0' shape=(2, 2) dtype=float32>, <tf.Tensor 'MatMul_1:0' shape=(2, 2) dtype=float32>]\n",
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
      "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
      "/job:localhost/replica:0/task:0/device:XLA_GPU:1 -> device: XLA_GPU device\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla K80, pci bus id: 0000:84:00.0, compute capability: 3.7\n",
      "/job:localhost/replica:0/task:0/device:GPU:1 -> device: 1, name: Tesla K80, pci bus id: 0000:85:00.0, compute capability: 3.7\n",
      "\n",
      "run c: [array([[22., 28.],\n",
      "       [49., 64.]], dtype=float32), array([[22., 28.],\n",
      "       [49., 64.]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "c = [] \n",
    "for d in ['/device:GPU:1', '/device:GPU:0']:   \n",
    "    with tf.device(d): \n",
    "        print(\"d:\",d)\n",
    "        a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3])     \n",
    "        b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2]) \n",
    "        c.append(tf.matmul(a, b)) \n",
    "        \n",
    "with tf.device('/cpu:0'): \n",
    "    print(\"c:\",c)\n",
    "    sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "    print(\"run c:\",sess.run([c[0],c[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 单机多卡完整栗子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "from datetime import datetime\n",
    "import os\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import mnist_inference\n",
    "\n",
    "# 定义训练神经网络时需要用到的参数。\n",
    "BATCH_SIZE = 100 \n",
    "LEARNING_RATE_BASE = 0.001\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "REGULARAZTION_RATE = 0.0001\n",
    "TRAINING_STEPS = 1000\n",
    "MOVING_AVERAGE_DECAY = 0.99 \n",
    "N_GPU = 2\n",
    "\n",
    "# 定义日志和模型输出的路径。\n",
    "MODEL_SAVE_PATH = \"logs_and_models/\"\n",
    "MODEL_NAME = \"model.ckpt\"\n",
    "\n",
    "# 定义数据存储的路径。因为需要为不同的GPU提供不同的训练数据，所以通过placerholder\n",
    "# 的方式就需要手动准备多份数据。为了方便训练数据的获取过程，可以采用第7章中介绍的Dataset\n",
    "# 的方式从TFRecord中读取数据。于是在这里提供的数据文件路径为将MNIST训练数据\n",
    "# 转化为TFRecords格式之后的路径。如何将MNIST数据转化为TFRecord格式在第7章中有\n",
    "# 详细介绍，这里不再赘述。\n",
    "DATA_PATH = \"output.tfrecords\" \n",
    "\n",
    "# 定义输入队列得到训练数据，具体细节可以参考第7章。\n",
    "def get_input():\n",
    "    \"\"\"\n",
    "    在TensorFlow 1.3中，Dataset API是放在contrib包中的：\n",
    "    tf.contrib.data.Dataset\n",
    "    而在TensorFlow 1.4中，Dataset API已经从contrib包中移除，变成了核心API的一员：\n",
    "    tf.data.Dataset\n",
    "    \"\"\"\n",
    "    dataset = tf.contrib.data.TFRecordDataset([DATA_PATH])\n",
    "\n",
    "    # 定义数据解析格式。\n",
    "    def parser(record):\n",
    "        features = tf.parse_single_example(\n",
    "            record,\n",
    "            features={\n",
    "                'image_raw': tf.FixedLenFeature([], tf.string),\n",
    "                'pixels': tf.FixedLenFeature([], tf.int64),\n",
    "                'label': tf.FixedLenFeature([], tf.int64),\n",
    "            })\n",
    "\n",
    "        # 解析图片和标签信息。\n",
    "        decoded_image = tf.decode_raw(features['image_raw'], tf.uint8)\n",
    "        reshaped_image = tf.reshape(decoded_image, [784])\n",
    "        retyped_image = tf.cast(reshaped_image, tf.float32)\n",
    "        label = tf.cast(features['label'], tf.int32)\n",
    "\n",
    "        return retyped_image, label\n",
    "\n",
    "    # 定义输入队列。\n",
    "    dataset = dataset.map(parser)\n",
    "    dataset = dataset.shuffle(buffer_size=10000)\n",
    "    dataset = dataset.repeat(10)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    \"\"\"\n",
    "    如何将这个dataset中的元素取出呢？方法是从Dataset中示例化一个Iterator，然后对Iterator进行迭代\n",
    "    \"\"\"\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "\n",
    "    features, labels = iterator.get_next()\n",
    "    return features, labels\n",
    "\n",
    "# 定义损失函数。对于给定的训练数据、正则化损失计算规则和命名空间，计算在这个命名空间\n",
    "# 下的总损失。之所以需要给定命名空间是因为不同的GPU上计算得出的正则化损失都会加入名为\n",
    "# loss的集合，如果不通过命名空间就会将不同GPU上的正则化损失都加进来。\n",
    "def get_loss(x, y_, regularizer, scope, reuse_variables=None):\n",
    "    # 沿用5.5节中定义的函数来计算神经网络的前向传播结果。\n",
    "    with tf.variable_scope(tf.get_variable_scope(), reuse=reuse_variables):\n",
    "        y = mnist_inference.inference(x, regularizer)\n",
    "    # 计算交叉熵损失。\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        logits=y, labels=y_))\n",
    "    # 计算当前GPU上计算得到的正则化损失。\n",
    "    regularization_loss = tf.add_n(tf.get_collection('losses', scope))\n",
    "    # 计算最终的总损失。\n",
    "    loss = cross_entropy + regularization_loss\n",
    "    return loss\n",
    "\n",
    "# 计算每一个变量梯度的平均值。\n",
    "def average_gradients(tower_grads):\n",
    "    average_grads = []\n",
    "\n",
    "    # 枚举所有的变量和变量在不同GPU上计算得出的梯度。\n",
    "    for grad_and_vars in zip(*tower_grads):\n",
    "        # 计算所有GPU上的梯度平均值。\n",
    "        grads = []\n",
    "        for g, _ in grad_and_vars:\n",
    "            expanded_g = tf.expand_dims(g, 0)\n",
    "            grads.append(expanded_g)\n",
    "        grad = tf.concat(grads, 0)\n",
    "        grad = tf.reduce_mean(grad, 0)\n",
    "\n",
    "        v = grad_and_vars[0][1]\n",
    "        grad_and_var = (grad, v)\n",
    "        # 将变量和它的平均梯度对应起来。\n",
    "        average_grads.append(grad_and_var)\n",
    "    # 返回所有变量的平均梯度，这个将被用于变量的更新。\n",
    "    return average_grads\n",
    "\n",
    "# 主训练过程。\n",
    "def main(argv=None): \n",
    "    # 将简单的运算放在CPU上，只有神经网络的训练过程放在GPU上。\n",
    "    with tf.Graph().as_default(), tf.device('/cpu:0'):\n",
    "        # 定义基本的训练过程\n",
    "        x, y_ = get_input()\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(REGULARAZTION_RATE)\n",
    "        \n",
    "        global_step = tf.get_variable('global_step', [], initializer=tf.constant_initializer(0), trainable=False)\n",
    "        learning_rate = tf.train.exponential_decay(\n",
    "            LEARNING_RATE_BASE, global_step, 60000 / BATCH_SIZE, LEARNING_RATE_DECAY)       \n",
    "        \n",
    "        opt = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        \n",
    "        tower_grads = []\n",
    "        reuse_variables = False\n",
    "        # 将神经网络的优化过程跑在不同的GPU上。\n",
    "        for i in range(N_GPU):\n",
    "            # 将优化过程指定在一个GPU上。\n",
    "            with tf.device('/gpu:%d' % i):\n",
    "                # 在gpu设备上执行前向传播、计算损失、计算梯度；\n",
    "                # 梯度存入内存list tower_grads中\n",
    "                with tf.name_scope('GPU_%d' % i) as scope:\n",
    "                    cur_loss = get_loss(x, y_, regularizer, scope, reuse_variables)\n",
    "                    # 在第一次声明变量之后，将控制变量重用的参数设置为True。这样可以\n",
    "                    # 让不同的GPU更新同一组参数。\n",
    "                    reuse_variables = True\n",
    "                    grads = opt.compute_gradients(cur_loss)\n",
    "                    tower_grads.append(grads)\n",
    "        \n",
    "        # 计算变量的平均梯度。\n",
    "        grads = average_gradients(tower_grads)\n",
    "        for grad, var in grads:\n",
    "            if grad is not None:\n",
    "            \ttf.summary.histogram('gradients_on_average/%s' % var.op.name, grad)\n",
    "\n",
    "        # 使用平均梯度更新参数。\n",
    "        apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n",
    "        for var in tf.trainable_variables():\n",
    "            tf.summary.histogram(var.op.name, var)\n",
    "\n",
    "        # 计算变量的滑动平均值。\n",
    "        variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    "        variables_to_average = (tf.trainable_variables() +tf.moving_average_variables())\n",
    "        variables_averages_op = variable_averages.apply(variables_to_average)\n",
    "        # 每一轮迭代需要更新变量的取值并更新变量的滑动平均值。\n",
    "        train_op = tf.group(apply_gradient_op, variables_averages_op)\n",
    "\n",
    "        saver = tf.train.Saver()\n",
    "        summary_op = tf.summary.merge_all()        \n",
    "        init = tf.global_variables_initializer()\n",
    "        with tf.Session(config=tf.ConfigProto(\n",
    "                allow_soft_placement=True, log_device_placement=True)) as sess:\n",
    "            # 初始化所有变量并启动队列。\n",
    "            init.run()\n",
    "            summary_writer = tf.summary.FileWriter(MODEL_SAVE_PATH, sess.graph)\n",
    "\n",
    "            for step in range(TRAINING_STEPS):\n",
    "                # 执行神经网络训练操作，并记录训练操作的运行时间。\n",
    "                start_time = time.time()\n",
    "                _, loss_value = sess.run([train_op, cur_loss])\n",
    "                duration = time.time() - start_time\n",
    "                \n",
    "                # 每隔一段时间数据当前的训练进度，并统计训练速度。\n",
    "                if step != 0 and step % 10 == 0:\n",
    "                    # 计算使用过的训练数据个数。因为在每一次运行训练操作时，每一个GPU\n",
    "                    # 都会使用一个batch的训练数据，所以总共用到的训练数据个数为\n",
    "                    # batch大小 × GPU个数。\n",
    "                    num_examples_per_step = BATCH_SIZE * N_GPU\n",
    "\n",
    "                    # num_examples_per_step为本次迭代使用到的训练数据个数，\n",
    "                    # duration为运行当前训练过程使用的时间，于是平均每秒可以处理的训\n",
    "                    # 练数据个数为num_examples_per_step / duration。\n",
    "                    examples_per_sec = num_examples_per_step / duration\n",
    "\n",
    "                    # duration为运行当前训练过程使用的时间，因为在每一个训练过程中，\n",
    "                    # 每一个GPU都会使用一个batch的训练数据，所以在单个batch上的训\n",
    "                    # 练所需要时间为duration / GPU个数。\n",
    "                    sec_per_batch = duration / N_GPU\n",
    "    \n",
    "                    # 输出训练信息。\n",
    "                    format_str = ('%s: step %d, loss = %.2f (%.1f examples/sec; %.3f sec/batch)')\n",
    "                    print (format_str % (datetime.now(), step, loss_value, examples_per_sec, sec_per_batch))\n",
    "                    \n",
    "                    # 通过TensorBoard可视化训练过程。\n",
    "                    summary = sess.run(summary_op)\n",
    "                    summary_writer.add_summary(summary, step)\n",
    "    \n",
    "                # 每隔一段时间保存当前的模型。\n",
    "                if step % 1000 == 0 or (step + 1) == TRAINING_STEPS:\n",
    "                    checkpoint_path = os.path.join(MODEL_SAVE_PATH, MODEL_NAME)\n",
    "                    saver.save(sess, checkpoint_path, global_step=step)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "\ttf.app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# distributeTensorflowExample\n",
    "\n",
    "## 分布式介绍中文文档\n",
    "```\n",
    "http://blog.csdn.net/luodongri/article/details/52596780\n",
    "```\n",
    "\n",
    "## 更多tensorflow和深度学习的内容，请参考我的书《tensorflow入门与实战》 \n",
    "```\n",
    "这本书深度学习入门的内容占了一半，都是很基础和入门的.\n",
    "如果刚入门的可以看看，自认为比网上看吴恩达的教程更容易看懂。\n",
    "如果是已经比较熟悉Tensorflow和深度学习了，可以不用看了。\n",
    "```\n",
    "链接：\n",
    "[https://item.jd.com/12307221.html](https://item.jd.com/12307221.html)\n",
    "\n",
    "## 说明\n",
    "\n",
    "```\n",
    "这是一个最简单的分布式tensorflow的例子。\n",
    "实现的功能是估计这个公式的2个参数：  Y = 2 * X + 10\n",
    "要估计的参数是weight是2， biasis 是10.\n",
    "程序执行的ps节点1个， worker节点2个。 执行命令示例在下面。\n",
    "详细关于tensorflow的分布式示例介绍：\n",
    "```\n",
    "\n",
    "## 执行命令示例\n",
    "\n",
    "```\n",
    "ps 节点执行： \n",
    "\n",
    "CUDA_VISIBLE_DEVICES='' python distribute.py --ps_hosts=192.168.100.42:2222 --worker_hosts=192.168.100.42:2224,192.168.100.253:2225 --job_name=ps --task_index=0\n",
    "\n",
    "worker 节点执行:\n",
    "\n",
    "CUDA_VISIBLE_DEVICES=0 python distribute.py --ps_hosts=192.168.100.42:2222 --worker_hosts=192.168.100.42:2224,192.168.100.253:2225 --job_name=worker --task_index=0\n",
    "\n",
    "CUDA_VISIBLE_DEVICES=0 python distribute.py --ps_hosts=192.168.100.42:2222 --worker_hosts=192.168.100.42:2224,192.168.100.253:2225 --job_name=worker --task_index=1\n",
    "\n",
    "```\n",
    "\n",
    "## Introduce\n",
    "\n",
    "```\n",
    "This is a most simple example for distributed tensorflow.\n",
    "\n",
    "The task is to estimate the paramters of the formula : Y = 2 * X + 10\n",
    "\n",
    "the paramter weight is the number 2, \n",
    "\n",
    "the paramter biasis is the number 10.\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## run example\n",
    "\n",
    "\n",
    "```\n",
    "ps server:\n",
    "\n",
    "CUDA_VISIBLE_DEVICES='' python distribute.py --ps_hosts=192.168.100.42:2222 --worker_hosts=192.168.100.42:2224,192.168.100.253:2225 --job_name=ps --task_index=0\n",
    "\n",
    "worker server:\n",
    "\n",
    "CUDA_VISIBLE_DEVICES=0 python distribute.py --ps_hosts=192.168.100.42:2222 --worker_hosts=192.168.100.42:2224,192.168.100.253:2225 --job_name=worker --task_index=0\n",
    "\n",
    "CUDA_VISIBLE_DEVICES=0 python distribute.py --ps_hosts=192.168.100.42:2222 --worker_hosts=192.168.100.42:2224,192.168.100.253:2225 --job_name=worker --task_index=1\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding=utf-8\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define parameters\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "tf.app.flags.DEFINE_float('learning_rate', 0.00003, 'Initial learning rate.')\n",
    "tf.app.flags.DEFINE_integer('steps_to_validate', 1000,\n",
    "                     'Steps to validate and print loss')\n",
    "\n",
    "# For distributed\n",
    "tf.app.flags.DEFINE_string(\"ps_hosts\", \"\",\n",
    "                           \"Comma-separated list of hostname:port pairs\")\n",
    "tf.app.flags.DEFINE_string(\"worker_hosts\", \"\",\n",
    "                           \"Comma-separated list of hostname:port pairs\")\n",
    "tf.app.flags.DEFINE_string(\"job_name\", \"\", \"One of 'ps', 'worker'\")\n",
    "tf.app.flags.DEFINE_integer(\"task_index\", 0, \"Index of task within the job\")\n",
    "tf.app.flags.DEFINE_integer(\"issync\", 0, \"是否采用分布式的同步模式，1表示同步模式，0表示异步模式\")\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = FLAGS.learning_rate\n",
    "steps_to_validate = FLAGS.steps_to_validate\n",
    "\n",
    "def main(_):\n",
    "  ps_hosts = FLAGS.ps_hosts.split(\",\")\n",
    "  worker_hosts = FLAGS.worker_hosts.split(\",\")\n",
    "  cluster = tf.train.ClusterSpec({\"ps\": ps_hosts, \"worker\": worker_hosts})\n",
    "  server = tf.train.Server(cluster,job_name=FLAGS.job_name,task_index=FLAGS.task_index)\n",
    "\n",
    "  issync = FLAGS.issync\n",
    "  if FLAGS.job_name == \"ps\":\n",
    "    server.join()\n",
    "  elif FLAGS.job_name == \"worker\":\n",
    "    with tf.device(tf.train.replica_device_setter(\n",
    "                    worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\n",
    "                    cluster=cluster)):\n",
    "      global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "      input = tf.placeholder(\"float\")\n",
    "      label = tf.placeholder(\"float\")\n",
    "\n",
    "      weight = tf.get_variable(\"weight\", [1], tf.float32, initializer=tf.random_normal_initializer())\n",
    "      biase  = tf.get_variable(\"biase\", [1], tf.float32, initializer=tf.random_normal_initializer())\n",
    "      pred = tf.multiply(input, weight) + biase\n",
    "\n",
    "      loss_value = loss(label, pred)\n",
    "      optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "\n",
    "      grads_and_vars = optimizer.compute_gradients(loss_value)\n",
    "      if issync == 1:\n",
    "        #同步模式计算更新梯度\n",
    "        rep_op = tf.train.SyncReplicasOptimizer(optimizer,\n",
    "                                                replicas_to_aggregate=len(\n",
    "                                                  worker_hosts),\n",
    "                                                replica_id=FLAGS.task_index,\n",
    "                                                total_num_replicas=len(\n",
    "                                                  worker_hosts),\n",
    "                                                use_locking=True)\n",
    "        train_op = rep_op.apply_gradients(grads_and_vars,\n",
    "                                       global_step=global_step)\n",
    "        init_token_op = rep_op.get_init_tokens_op()\n",
    "        chief_queue_runner = rep_op.get_chief_queue_runner()\n",
    "      else:\n",
    "        #异步模式计算更新梯度\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars,\n",
    "                                       global_step=global_step)\n",
    "\n",
    "\n",
    "      init_op = tf.initialize_all_variables()\n",
    "      \n",
    "      saver = tf.train.Saver()\n",
    "      tf.summary.scalar('cost', loss_value)\n",
    "      summary_op = tf.summary.merge_all()\n",
    " \n",
    "    sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),\n",
    "                            logdir=\"./checkpoint/\",\n",
    "                            init_op=init_op,\n",
    "                            summary_op=None,\n",
    "                            saver=saver,\n",
    "                            global_step=global_step,\n",
    "                            save_model_secs=60)\n",
    "\n",
    "    with sv.prepare_or_wait_for_session(server.target) as sess:\n",
    "      # 如果是同步模式\n",
    "      if FLAGS.task_index == 0 and issync == 1:\n",
    "        sv.start_queue_runners(sess, [chief_queue_runner])\n",
    "        sess.run(init_token_op)\n",
    "      step = 0\n",
    "      while  step < 1000000:\n",
    "        train_x = np.random.randn(1)\n",
    "        train_y = 2 * train_x + np.random.randn(1) * 0.33  + 10\n",
    "        _, loss_v, step = sess.run([train_op, loss_value,global_step], feed_dict={input:train_x, label:train_y})\n",
    "        if step % steps_to_validate == 0:\n",
    "          w,b = sess.run([weight,biase])\n",
    "          print(\"step: %d, weight: %f, biase: %f, loss: %f\" %(step, w, b, loss_v))\n",
    "\n",
    "    sv.stop()\n",
    "\n",
    "def loss(label, pred):\n",
    "  return tf.square(label - pred)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  tf.app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ps 节点执行：\n",
    "python distributed.py --job_name=ps --task_index=0\n",
    "\n",
    "### worker1 节点执行：\n",
    "python distributed.py --job_name=worker --task_index=0\n",
    "\n",
    "### worker2 节点执行：\n",
    "python distributed.py --job_name=worker --task_index=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding:utf-8\n",
    "import math\n",
    "import tempfile\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "flags = tf.app.flags\n",
    "IMAGE_PIXELS = 28\n",
    "# 定义默认训练参数和数据路径\n",
    "flags.DEFINE_string('data_dir', '/tmp/mnist-data', 'Directory  for storing mnist data')\n",
    "flags.DEFINE_integer('hidden_units', 100, 'Number of units in the hidden layer of the NN')\n",
    "flags.DEFINE_integer('train_steps', 10000, 'Number of training steps to perform')\n",
    "flags.DEFINE_integer('batch_size', 100, 'Training batch size ')\n",
    "flags.DEFINE_float('learning_rate', 0.01, 'Learning rate')\n",
    "# 定义分布式参数\n",
    "# 参数服务器parameter server节点\n",
    "flags.DEFINE_string('ps_hosts', '192.168.32.145:22221', 'Comma-separated list of hostname:port pairs')\n",
    "# 两个worker节点\n",
    "flags.DEFINE_string('worker_hosts', '192.168.32.146:22221,192.168.32.160:22221',\n",
    "                    'Comma-separated list of hostname:port pairs')\n",
    "# 设置job name参数\n",
    "flags.DEFINE_string('job_name', None, 'job name: worker or ps')\n",
    "# 设置任务的索引\n",
    "flags.DEFINE_integer('task_index', None, 'Index of task within the job')\n",
    "# 选择异步并行，同步并行\n",
    "flags.DEFINE_integer(\"issync\", None, \"是否采用分布式的同步模式，1表示同步模式，0表示异步模式\")\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "\n",
    "def main(unused_argv):\n",
    "    mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)\n",
    "\n",
    "    if FLAGS.job_name is None or FLAGS.job_name == '':\n",
    "        raise ValueError('Must specify an explicit job_name !')\n",
    "    else:\n",
    "        print 'job_name : %s' % FLAGS.job_name\n",
    "    if FLAGS.task_index is None or FLAGS.task_index == '':\n",
    "        raise ValueError('Must specify an explicit task_index!')\n",
    "    else:\n",
    "        print 'task_index : %d' % FLAGS.task_index\n",
    "\n",
    "    ps_spec = FLAGS.ps_hosts.split(',')\n",
    "    worker_spec = FLAGS.worker_hosts.split(',')\n",
    "\n",
    "    # 创建集群\n",
    "    num_worker = len(worker_spec)\n",
    "    cluster = tf.train.ClusterSpec({'ps': ps_spec, 'worker': worker_spec})\n",
    "    server = tf.train.Server(cluster, job_name=FLAGS.job_name, task_index=FLAGS.task_index)\n",
    "    if FLAGS.job_name == 'ps':\n",
    "        server.join()\n",
    "\n",
    "    is_chief = (FLAGS.task_index == 0)\n",
    "    # worker_device = '/job:worker/task%d/cpu:0' % FLAGS.task_index\n",
    "    with tf.device(tf.train.replica_device_setter(\n",
    "            cluster=cluster\n",
    "    )):\n",
    "        global_step = tf.Variable(0, name='global_step', trainable=False)  # 创建纪录全局训练步数变量\n",
    "\n",
    "        hid_w = tf.Variable(tf.truncated_normal([IMAGE_PIXELS * IMAGE_PIXELS, FLAGS.hidden_units],\n",
    "                                                stddev=1.0 / IMAGE_PIXELS), name='hid_w')\n",
    "        hid_b = tf.Variable(tf.zeros([FLAGS.hidden_units]), name='hid_b')\n",
    "\n",
    "        sm_w = tf.Variable(tf.truncated_normal([FLAGS.hidden_units, 10],\n",
    "                                               stddev=1.0 / math.sqrt(FLAGS.hidden_units)), name='sm_w')\n",
    "        sm_b = tf.Variable(tf.zeros([10]), name='sm_b')\n",
    "\n",
    "        x = tf.placeholder(tf.float32, [None, IMAGE_PIXELS * IMAGE_PIXELS])\n",
    "        y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "        hid_lin = tf.nn.xw_plus_b(x, hid_w, hid_b)\n",
    "        hid = tf.nn.relu(hid_lin)\n",
    "\n",
    "        y = tf.nn.softmax(tf.nn.xw_plus_b(hid, sm_w, sm_b))\n",
    "        cross_entropy = -tf.reduce_sum(y_ * tf.log(tf.clip_by_value(y, 1e-10, 1.0)))\n",
    "\n",
    "        opt = tf.train.AdamOptimizer(FLAGS.learning_rate)\n",
    "\n",
    "        train_step = opt.minimize(cross_entropy, global_step=global_step)\n",
    "        # 生成本地的参数初始化操作init_op\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        train_dir = tempfile.mkdtemp()\n",
    "        sv = tf.train.Supervisor(is_chief=is_chief, logdir=train_dir, init_op=init_op, recovery_wait_secs=1,\n",
    "                                 global_step=global_step)\n",
    "\n",
    "        if is_chief:\n",
    "            print 'Worker %d: Initailizing session...' % FLAGS.task_index\n",
    "        else:\n",
    "            print 'Worker %d: Waiting for session to be initaialized...' % FLAGS.task_index\n",
    "        sess = sv.prepare_or_wait_for_session(server.target)\n",
    "        print 'Worker %d: Session initialization  complete.' % FLAGS.task_index\n",
    "\n",
    "        time_begin = time.time()\n",
    "        print 'Traing begins @ %f' % time_begin\n",
    "\n",
    "        local_step = 0\n",
    "        while True:\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(FLAGS.batch_size)\n",
    "            train_feed = {x: batch_xs, y_: batch_ys}\n",
    "\n",
    "            _, step = sess.run([train_step, global_step], feed_dict=train_feed)\n",
    "            local_step += 1\n",
    "\n",
    "            now = time.time()\n",
    "            print '%f: Worker %d: traing step %d dome (global step:%d)' % (now, FLAGS.task_index, local_step, step)\n",
    "\n",
    "            if step >= FLAGS.train_steps:\n",
    "                break\n",
    "\n",
    "        time_end = time.time()\n",
    "        print 'Training ends @ %f' % time_end\n",
    "        train_time = time_end - time_begin\n",
    "        print 'Training elapsed time:%f s' % train_time\n",
    "\n",
    "        val_feed = {x: mnist.validation.images, y_: mnist.validation.labels}\n",
    "        val_xent = sess.run(cross_entropy, feed_dict=val_feed)\n",
    "        print 'After %d training step(s), validation cross entropy = %g' % (FLAGS.train_steps, val_xent)\n",
    "    sess.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\"\"\"Distributed MNIST training and validation, with model replicas.\n",
    "\n",
    "A simple softmax model with one hidden layer is defined. The parameters\n",
    "(weights and biases) are located on one parameter server (ps), while the ops\n",
    "are executed on two worker nodes by default. The TF sessions also run on the\n",
    "worker node.\n",
    "Multiple invocations of this script can be done in parallel, with different\n",
    "values for --task_index. There should be exactly one invocation with\n",
    "--task_index, which will create a master session that carries out variable\n",
    "initialization. The other, non-master, sessions will wait for the master\n",
    "session to finish the initialization before proceeding to the training stage.\n",
    "\n",
    "The coordination between the multiple worker invocations occurs due to\n",
    "the definition of the parameters on the same ps devices. The parameter updates\n",
    "from one worker is visible to all other workers. As such, the workers can\n",
    "perform forward computation and gradient calculation in parallel, which\n",
    "should lead to increased training speed for the simple model.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import tempfile\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "flags = tf.app.flags\n",
    "flags.DEFINE_string(\"data_dir\", \"/tmp/mnist-data\",\n",
    "                    \"Directory for storing mnist data\")\n",
    "flags.DEFINE_boolean(\"download_only\", False,\n",
    "                     \"Only perform downloading of data; Do not proceed to \"\n",
    "                     \"session preparation, model definition or training\")\n",
    "flags.DEFINE_integer(\"task_index\", None,\n",
    "                     \"Worker task index, should be >= 0. task_index=0 is \"\n",
    "                     \"the master worker task the performs the variable \"\n",
    "                     \"initialization \")\n",
    "flags.DEFINE_integer(\"num_gpus\", 1, \"Total number of gpus for each machine.\"\n",
    "                     \"If you don't use GPU, please set it to '0'\")\n",
    "flags.DEFINE_integer(\"replicas_to_aggregate\", None,\n",
    "                     \"Number of replicas to aggregate before parameter update\"\n",
    "                     \"is applied (For sync_replicas mode only; default: \"\n",
    "                     \"num_workers)\")\n",
    "flags.DEFINE_integer(\"hidden_units\", 100,\n",
    "                     \"Number of units in the hidden layer of the NN\")\n",
    "flags.DEFINE_integer(\"train_steps\", 20000,\n",
    "                     \"Number of (global) training steps to perform\")\n",
    "flags.DEFINE_integer(\"batch_size\", 100, \"Training batch size\")\n",
    "flags.DEFINE_float(\"learning_rate\", 0.01, \"Learning rate\")\n",
    "flags.DEFINE_boolean(\n",
    "    \"sync_replicas\", False,\n",
    "    \"Use the sync_replicas (synchronized replicas) mode, \"\n",
    "    \"wherein the parameter updates from workers are aggregated \"\n",
    "    \"before applied to avoid stale gradients\")\n",
    "flags.DEFINE_boolean(\n",
    "    \"existing_servers\", False, \"Whether servers already exists. If True, \"\n",
    "    \"will use the worker hosts via their GRPC URLs (one client process \"\n",
    "    \"per worker host). Otherwise, will create an in-process TensorFlow \"\n",
    "    \"server.\")\n",
    "flags.DEFINE_string(\"ps_hosts\", \"localhost:2222\",\n",
    "                    \"Comma-separated list of hostname:port pairs\")\n",
    "flags.DEFINE_string(\"worker_hosts\", \"localhost:2223,localhost:2224\",\n",
    "                    \"Comma-separated list of hostname:port pairs\")\n",
    "flags.DEFINE_string(\"job_name\", None, \"job name: worker or ps\")\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "IMAGE_PIXELS = 28\n",
    "\n",
    "# Example:\n",
    "#   cluster = {'ps': ['host1:2222', 'host2:2222'],\n",
    "#              'worker': ['host3:2222', 'host4:2222', 'host5:2222']}\n",
    "#   os.environ['TF_CONFIG'] = json.dumps(\n",
    "#       {'cluster': cluster,\n",
    "#        'task': {'type': 'worker', 'index': 1}})\n",
    "\n",
    "def main(unused_argv):\n",
    "  # Parse environment variable TF_CONFIG to get job_name and task_index\n",
    "\n",
    "  # If not explicitly specified in the constructor and the TF_CONFIG\n",
    "  # environment variable is present, load cluster_spec from TF_CONFIG.\n",
    "  tf_config = json.loads(os.environ.get('TF_CONFIG') or '{}')\n",
    "  task_config = tf_config.get('task', {})\n",
    "  task_type = task_config.get('type')\n",
    "  task_index = task_config.get('index')\n",
    "\n",
    "  FLAGS.job_name = task_type\n",
    "  FLAGS.task_index = task_index\n",
    "\n",
    "  mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)\n",
    "  if FLAGS.download_only:\n",
    "    sys.exit(0)\n",
    "\n",
    "  if FLAGS.job_name is None or FLAGS.job_name == \"\":\n",
    "    raise ValueError(\"Must specify an explicit `job_name`\")\n",
    "  if FLAGS.task_index is None or FLAGS.task_index == \"\":\n",
    "    raise ValueError(\"Must specify an explicit `task_index`\")\n",
    "\n",
    "  print(\"job name = %s\" % FLAGS.job_name)\n",
    "  print(\"task index = %d\" % FLAGS.task_index)\n",
    "\n",
    "  cluster_config = tf_config.get('cluster', {})\n",
    "  ps_hosts = cluster_config.get('ps')\n",
    "  worker_hosts = cluster_config.get('worker')\n",
    "\n",
    "  ps_hosts_str = ','.join(ps_hosts)\n",
    "  worker_hosts_str = ','.join(worker_hosts)\n",
    "\n",
    "  FLAGS.ps_hosts = ps_hosts_str\n",
    "  FLAGS.worker_hosts = worker_hosts_str\n",
    "\n",
    "  # Construct the cluster and start the server\n",
    "  ps_spec = FLAGS.ps_hosts.split(\",\")\n",
    "  worker_spec = FLAGS.worker_hosts.split(\",\")\n",
    "\n",
    "  # Get the number of workers.\n",
    "  num_workers = len(worker_spec)\n",
    "\n",
    "  cluster = tf.train.ClusterSpec({\"ps\": ps_spec, \"worker\": worker_spec})\n",
    "\n",
    "  if not FLAGS.existing_servers:\n",
    "    # Not using existing servers. Create an in-process server.\n",
    "    server = tf.train.Server(\n",
    "        cluster, job_name=FLAGS.job_name, task_index=FLAGS.task_index)\n",
    "    if FLAGS.job_name == \"ps\":\n",
    "      server.join()\n",
    "\n",
    "  is_chief = (FLAGS.task_index == 0)\n",
    "  if FLAGS.num_gpus > 0:\n",
    "    # Avoid gpu allocation conflict: now allocate task_num -> #gpu\n",
    "    # for each worker in the corresponding machine\n",
    "    gpu = (FLAGS.task_index % FLAGS.num_gpus)\n",
    "    worker_device = \"/job:worker/task:%d/gpu:%d\" % (FLAGS.task_index, gpu)\n",
    "  elif FLAGS.num_gpus == 0:\n",
    "    # Just allocate the CPU to worker server\n",
    "    cpu = 0\n",
    "    worker_device = \"/job:worker/task:%d/cpu:%d\" % (FLAGS.task_index, cpu)\n",
    "  # The device setter will automatically place Variables ops on separate\n",
    "  # parameter servers (ps). The non-Variable ops will be placed on the workers.\n",
    "  # The ps use CPU and workers use corresponding GPU\n",
    "  with tf.device(\n",
    "      tf.train.replica_device_setter(\n",
    "          worker_device=worker_device,\n",
    "          ps_device=\"/job:ps/cpu:0\",\n",
    "          cluster=cluster)):\n",
    "    global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "\n",
    "    # Variables of the hidden layer\n",
    "    hid_w = tf.Variable(\n",
    "        tf.truncated_normal(\n",
    "            [IMAGE_PIXELS * IMAGE_PIXELS, FLAGS.hidden_units],\n",
    "            stddev=1.0 / IMAGE_PIXELS),\n",
    "        name=\"hid_w\")\n",
    "    hid_b = tf.Variable(tf.zeros([FLAGS.hidden_units]), name=\"hid_b\")\n",
    "\n",
    "    # Variables of the softmax layer\n",
    "    sm_w = tf.Variable(\n",
    "        tf.truncated_normal(\n",
    "            [FLAGS.hidden_units, 10],\n",
    "            stddev=1.0 / math.sqrt(FLAGS.hidden_units)),\n",
    "        name=\"sm_w\")\n",
    "    sm_b = tf.Variable(tf.zeros([10]), name=\"sm_b\")\n",
    "\n",
    "    # Ops: located on the worker specified with FLAGS.task_index\n",
    "    x = tf.placeholder(tf.float32, [None, IMAGE_PIXELS * IMAGE_PIXELS])\n",
    "    y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "    hid_lin = tf.nn.xw_plus_b(x, hid_w, hid_b)\n",
    "    hid = tf.nn.relu(hid_lin)\n",
    "\n",
    "    y = tf.nn.softmax(tf.nn.xw_plus_b(hid, sm_w, sm_b))\n",
    "    cross_entropy = -tf.reduce_sum(y_ * tf.log(tf.clip_by_value(y, 1e-10, 1.0)))\n",
    "\n",
    "    opt = tf.train.AdamOptimizer(FLAGS.learning_rate)\n",
    "\n",
    "    if FLAGS.sync_replicas:\n",
    "      if FLAGS.replicas_to_aggregate is None:\n",
    "        replicas_to_aggregate = num_workers\n",
    "      else:\n",
    "        replicas_to_aggregate = FLAGS.replicas_to_aggregate\n",
    "\n",
    "      opt = tf.train.SyncReplicasOptimizer(\n",
    "          opt,\n",
    "          replicas_to_aggregate=replicas_to_aggregate,\n",
    "          total_num_replicas=num_workers,\n",
    "          name=\"mnist_sync_replicas\")\n",
    "\n",
    "    train_step = opt.minimize(cross_entropy, global_step=global_step)\n",
    "\n",
    "    if FLAGS.sync_replicas:\n",
    "      local_init_op = opt.local_step_init_op\n",
    "      if is_chief:\n",
    "        local_init_op = opt.chief_init_op\n",
    "\n",
    "      ready_for_local_init_op = opt.ready_for_local_init_op\n",
    "\n",
    "      # Initial token and chief queue runners required by the sync_replicas mode\n",
    "      chief_queue_runner = opt.get_chief_queue_runner()\n",
    "      sync_init_op = opt.get_init_tokens_op()\n",
    "\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    train_dir = tempfile.mkdtemp()\n",
    "\n",
    "    if FLAGS.sync_replicas:\n",
    "      sv = tf.train.Supervisor(\n",
    "          is_chief=is_chief,\n",
    "          logdir=train_dir,\n",
    "          init_op=init_op,\n",
    "          ####\n",
    "          local_init_op=local_init_op,\n",
    "          ready_for_local_init_op=ready_for_local_init_op,\n",
    "          ####\n",
    "          recovery_wait_secs=1,\n",
    "          global_step=global_step)\n",
    "    else:\n",
    "      sv = tf.train.Supervisor(\n",
    "          is_chief=is_chief,\n",
    "          logdir=train_dir,\n",
    "          init_op=init_op,\n",
    "          recovery_wait_secs=1,\n",
    "          global_step=global_step)\n",
    "\n",
    "    sess_config = tf.ConfigProto(\n",
    "        allow_soft_placement=True,\n",
    "        log_device_placement=False,\n",
    "        device_filters=[\"/job:ps\",\n",
    "                        \"/job:worker/task:%d\" % FLAGS.task_index])\n",
    "\n",
    "    # The chief worker (task_index==0) session will prepare the session,\n",
    "    # while the remaining workers will wait for the preparation to complete.\n",
    "    if is_chief:\n",
    "      print(\"Worker %d: Initializing session...\" % FLAGS.task_index)\n",
    "    else:\n",
    "      print(\"Worker %d: Waiting for session to be initialized...\" %\n",
    "            FLAGS.task_index)\n",
    "\n",
    "    if FLAGS.existing_servers:\n",
    "      server_grpc_url = \"grpc://\" + worker_spec[FLAGS.task_index]\n",
    "      print(\"Using existing server at: %s\" % server_grpc_url)\n",
    "\n",
    "      sess = sv.prepare_or_wait_for_session(server_grpc_url, config=sess_config)\n",
    "    else:\n",
    "      sess = sv.prepare_or_wait_for_session(server.target, config=sess_config)\n",
    "\n",
    "    print(\"Worker %d: Session initialization complete.\" % FLAGS.task_index)\n",
    "\n",
    "    if FLAGS.sync_replicas and is_chief:\n",
    "      # Chief worker will start the chief queue runner and call the init op.\n",
    "      sess.run(sync_init_op)\n",
    "      sv.start_queue_runners(sess, [chief_queue_runner])\n",
    "\n",
    "    # Perform training\n",
    "    time_begin = time.time()\n",
    "    print(\"Training begins @ %f\" % time_begin)\n",
    "\n",
    "    local_step = 0\n",
    "    while True:\n",
    "      # Training feed\n",
    "      batch_xs, batch_ys = mnist.train.next_batch(FLAGS.batch_size)\n",
    "      train_feed = {x: batch_xs, y_: batch_ys}\n",
    "\n",
    "      _, step = sess.run([train_step, global_step], feed_dict=train_feed)\n",
    "      local_step += 1\n",
    "\n",
    "      now = time.time()\n",
    "      print(\"%f: Worker %d: training step %d done (global step: %d)\" %\n",
    "            (now, FLAGS.task_index, local_step, step))\n",
    "\n",
    "      if step >= FLAGS.train_steps:\n",
    "        break\n",
    "\n",
    "    time_end = time.time()\n",
    "    print(\"Training ends @ %f\" % time_end)\n",
    "    training_time = time_end - time_begin\n",
    "    print(\"Training elapsed time: %f s\" % training_time)\n",
    "\n",
    "    # Validation feed\n",
    "    val_feed = {x: mnist.validation.images, y_: mnist.validation.labels}\n",
    "    val_xent = sess.run(cross_entropy, feed_dict=val_feed)\n",
    "    print(\"After %d training step(s), validation cross entropy = %g\" %\n",
    "          (FLAGS.train_steps, val_xent))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  tf.app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 由于历史原因,图中复制的例子不多([Yaroslav’s gist](https://gist.github.com/yaroslavvb/ef407a599f0f549f62d91c3a00dcfb6c)是一个例外).使用图中复制的程序通常包括为每个工作者创建相同图形结构的循环(例如line 74 of the gist上的循环),并使用工作者之间的变量共享.\n",
    "1. 图中复制持续存在的一个地方是在单个进程中使用多个设备(例如,多个GPU). CIFAR-10 example model for multiple GPUs是此模式的一个示例(请参阅GPU设备here上的循环)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Example of barrier implementation using TensorFlow shared variables.\n",
    "All workers synchronize on barrier, copy global parameters to local versions\n",
    "and increment global parameter variable asynchronously. Should see something\n",
    "like this:\n",
    "bash> killall python\n",
    "bash> python simple_barrier.py --num_workers=4\n",
    "worker  0, local_param  4 global_param  5\n",
    "worker  2, local_param  4 global_param  7\n",
    "worker  1, local_param  4 global_param  7\n",
    "worker  3, local_param  4 global_param  8\n",
    "worker  3, local_param  8 global_param 10\n",
    "worker  2, local_param  8 global_param 11\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import threading\n",
    "import time\n",
    "\n",
    "flags = tf.flags\n",
    "flags.DEFINE_integer(\"iters\", 10, \"Maximum number of steps\")\n",
    "flags.DEFINE_integer(\"starting_port\", \"12222\", \"port of first worker\")\n",
    "flags.DEFINE_integer(\"num_workers\", 4, \"number of workers\")\n",
    "flags.DEFINE_integer(\"task\", -1, \"internal use\")\n",
    "flags.DEFINE_float(\"sleep_interval\", 0.1, \"how long to sleep in wait loop\")\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "# setup local cluster from flags\n",
    "host = \"127.0.0.1:\"\n",
    "s = FLAGS.starting_port\n",
    "N = FLAGS.num_workers\n",
    "cluster = {\"worker\": [host+str(port) for port in range(s, s+N)]}\n",
    "clusterspec = tf.train.ClusterSpec(cluster).as_cluster_def()\n",
    "\n",
    "# global ops\n",
    "init_op = None\n",
    "train_ops = []       # worker local train ops, read local params, update global\n",
    "counter_vars = []    # counters for barrier\n",
    "counter_adder_ops = []\n",
    "global_param_var = None\n",
    "local_param_vars = []\n",
    "local_param_sync_ops = []\n",
    "\n",
    "def default_config():\n",
    "  optimizer_options = tf.OptimizerOptions(opt_level=tf.OptimizerOptions.L0)\n",
    "  config = tf.ConfigProto(\n",
    "    graph_options=tf.GraphOptions(optimizer_options=optimizer_options))\n",
    "  config.log_device_placement = False\n",
    "  config.allow_soft_placement = False\n",
    "  return config\n",
    "\n",
    "def create_graph(devices):\n",
    "  \"\"\"Create graph that keeps global params + counters on devices[0] and\n",
    "  local params/train ops on devices[:]\"\"\"\n",
    "\n",
    "  global train_ops, counter_vars, counter_adder_ops, global_param_var, local_param_vars, local_param_sync_ops\n",
    "\n",
    "  dtype=tf.int32\n",
    "\n",
    "  with tf.device(devices[0]):\n",
    "    global_param_var = tf.get_variable(\"param\", shape=(), dtype=dtype,\n",
    "                                       initializer=tf.zeros_initializer)\n",
    "    for i in range(2):\n",
    "      counter_var = tf.get_variable(\"counter-\"+str(i), (), tf.int32,\n",
    "                                    initializer=tf.zeros_initializer)\n",
    "      counter_vars.append(counter_var)\n",
    "      counter_adder_ops.append(counter_var.assign_add(1, use_locking=True))\n",
    "\n",
    "  # create local version of parameters\n",
    "  for (i, device) in enumerate(devices):\n",
    "    with tf.device(device):\n",
    "      local_param_var = tf.get_variable(\"local_param-\"+str(i), (), dtype,\n",
    "                                        initializer=tf.zeros_initializer)\n",
    "      local_param_vars.append(local_param_var)\n",
    "      \n",
    "      local_param_sync_op = local_param_var.assign(global_param_var)\n",
    "      local_param_sync_ops.append(local_param_sync_op)\n",
    "      train_op = global_param_var.assign_add(1)\n",
    "      train_ops.append(train_op)\n",
    "\n",
    "      \n",
    "  init_op = tf.initialize_all_variables()\n",
    "  return (init_op, train_ops)\n",
    "\n",
    "\n",
    "def create_worker_threads(sess):\n",
    "  \"\"\"Creates a thread for each op in ops, running it iters times.\"\"\"\n",
    "\n",
    "  def barrier():\n",
    "    sess.run(counter_adder_ops[0])\n",
    "    while sess.run(counter_vars[0]) % N != 0:\n",
    "      time.sleep(FLAGS.sleep_interval)\n",
    "    sess.run(counter_adder_ops[1])\n",
    "    while sess.run(counter_vars[1]) % N != 0:\n",
    "      time.sleep(FLAGS.sleep_interval)\n",
    "    \n",
    "  def create_run_method(worker_id):\n",
    "    def _run():\n",
    "      local_param_var = local_param_vars[worker_id]\n",
    "      sync_op = local_param_sync_ops[worker_id]\n",
    "      train_op = train_ops[worker_id]\n",
    "      for i in range(FLAGS.iters):\n",
    "        barrier()\n",
    "        sess.run(sync_op)\n",
    "        barrier()\n",
    "        old_val, updated_val = sess.run([local_param_var, train_op])\n",
    "        print(\"worker %2d, local_param %2d global_param %2d\" %(worker_id,\n",
    "                                                               old_val,\n",
    "                                                               updated_val))\n",
    "    return _run\n",
    "\n",
    "  return [threading.Thread(target=create_run_method(i))\n",
    "          for i in range(N)]\n",
    "\n",
    "\n",
    "def wait_for_threads_to_finish(threads):\n",
    "  while any(t.is_alive() for t in threads):\n",
    "    time.sleep(FLAGS.sleep_interval)\n",
    "\n",
    "\n",
    "def launch_workers():\n",
    "  \"\"\"Launch processes running TensorFlow servers.\"\"\"\n",
    "  \n",
    "  def runcmd(cmd): subprocess.Popen(cmd, shell=True, stderr=subprocess.STDOUT)\n",
    "  for i in range(N):\n",
    "    cmd = \"python simple_barrier.py --task=\"+str(i)\n",
    "    print(\"Executing \"+cmd)\n",
    "    runcmd(cmd)\n",
    "\n",
    "def run_worker(task=-1):\n",
    "  print(\"Worker %d entering server loop\" %(task))\n",
    "  server = tf.train.Server(clusterspec, config=default_config(),\n",
    "                           job_name=\"worker\",\n",
    "                           task_index=FLAGS.task)\n",
    "  server.join()\n",
    "\n",
    "def run_client():\n",
    "  tasks = [\"/job:worker/task:%d\"%(i) for i in range(FLAGS.num_workers)]\n",
    "\n",
    "  (init_op, add_ops) = create_graph(tasks)\n",
    "  \n",
    "  # launch distributed service\n",
    "  print(\"launching workers\")\n",
    "  launch_workers()\n",
    "\n",
    "  # reset containers of first worker (it stores shared state)\n",
    "  worker_ip = host+str(FLAGS.starting_port)\n",
    "\n",
    "  # need tf.Session.reset if there are worker servers launched from before\n",
    "  # However, tf.Session.reset can hang if workers are in process of being\n",
    "  # brought up, hence more robust to do killall python\n",
    "  #  tf.Session.reset(\"grpc://\" + worker_ip)\n",
    "  print(\"Creating session\")\n",
    "  sess = tf.Session(\"grpc://\"+ worker_ip,\n",
    "                    config=default_config())\n",
    "  sess.run(init_op)\n",
    "  \n",
    "  worker_threads = create_worker_threads(sess)\n",
    "  [t.start() for t in worker_threads]\n",
    "  wait_for_threads_to_finish(worker_threads)\n",
    "\n",
    "if __name__=='__main__':\n",
    "  if FLAGS.task == -1:\n",
    "    # client launches worker processes and issues .run calls\n",
    "    run_client()\n",
    "  else:\n",
    "    run_worker(FLAGS.task)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
